######################################################################
## Dependencies ##
from celseq2.helper import mkfolder, join_path, base_name, dir_name, print_logger
from celseq2.helper import is_nonempty_file, resetfolder, rmfolder
from celseq2.helper import cook_sample_sheet
from celseq2.prepare_annotation_model import cook_anno_model
from celseq2.count_umi import count_umi
import pandas as pd
import glob
import pickle
from collections import Counter

import shutil
import os

## Inforamtion ##
PROJECT_NAME = config.get('PROJECT_NAME', None)  # 'celseq2_demo'
# '/ifs/home/yy1533/Lab/cel-seq-pipe/demo/celseq2'
DIR_PROJ = config.get('DIR_PROJ', None)

## Sample Sheet ##
SAMPLE_SHEET = cook_sample_sheet(config.get('SAMPLE_SHEET', None))  # ''

## CEL-seq2 Tech Setting ##
# '/ifs/data/yanailab/refs/barcodes/barcodes_cel-seq_umis96.tab'
BC_INDEX_FPATH = config.get('BC_INDEX_FPATH', None)
BC_IDs_DEFAULT = config.get('BC_IDs_DEFAULT', None)  # '1-96'
UMI_LENGTH = config.get('UMI_LENGTH', None)  # 6
BC_LENGTH = config.get('BC_LENGTH', None)  # 6

## Tools ##
# '/ifs/data/yanailab/refs/danio_rerio/danRer10_87/genome/Danio_rerio.GRCz10.dna.toplevel'
BOWTIE2_INDEX_PREFIX = config.get('BOWTIE2_INDEX_PREFIX', None)
BOWTIE2 = config.get('BOWTIE2', None)  # '/local/apps/bowtie2/2.3.1/bowtie2'

## Annotations ##
# '/ifs/data/yanailab/refs/danio_rerio/danRer10_87/gtf/Danio_rerio.GRCz10.87.gtf.gz'
GFF = config.get('GFF', None)

## Demultiplexing ##
FASTQ_QUAL_MIN_OF_BC = config.get('FASTQ_QUAL_MIN_OF_BC', None)  # 10
CUT_LENGTH = config.get('CUT_LENGTH', None)  # 35
## Alignment ##
ALIGNER = config.get('ALIGNER', None)  # 'bowtie2'

## UMI Count ##
ALN_QUAL_MIN = config.get('ALN_QUAL_MIN', None)  # 0


## Running Parameters ##
# bc_ids_used=config.get('bc_ids_used', None)  # '1,2,3,4-5'
num_threads = config.get('num_threads', None)  # 5
verbose = config.get('verbose', None)  # True


#######################
## Pipeline reserved ##
#######################
item_names = list(SAMPLE_SHEET.index)
# expand('{r1_fpath}', r1_fpath=SAMPLE_SHEET['R1'].values)
sample_list = SAMPLE_SHEET['SAMPLE_NAME'].values
# expand('{r2_fpath}', r2_fpath=SAMPLE_SHEET['R2'].values)
R1 = SAMPLE_SHEET['R1'].values
R2 = SAMPLE_SHEET['R2'].values
bc_used = SAMPLE_SHEET['CELL_BARCODES_INDEX'].values


if not DIR_PROJ:
    print_logger('Please specify configuration for pipeline\n')
    exit(1)
if R1 is None or R2 is None:
    print_logger('Please specify reads file\n')
    exit(1)

SUBDIR_INPUT = 'input'
SUBDIR_FASTQ = 'small_fq'
SUBDIR_ALIGN = 'small_sam'
SUBDIR_UMI_CNT = 'small_umi_count'
SUBDIR_UMI_SET = 'small_umi_set'
SUBDIR_EXPR = 'expr'
SUBDIR_LOG = 'small_log'
SUBDIR_QSUB = 'qsub_log'
SUBDIR_DIAG = 'small_diagnose'
SUBDIR_ANNO = 'annotation'
SUBDIRS = [SUBDIR_INPUT,
           SUBDIR_FASTQ, SUBDIR_ALIGN, SUBDIR_UMI_CNT, SUBDIR_UMI_SET,
           SUBDIR_EXPR,
           SUBDIR_LOG, SUBDIR_QSUB, SUBDIR_DIAG, SUBDIR_ANNO
           ]

aln_diagnose_item = ["_unmapped",
                     "_low_map_qual", '_multimapped', "_uniquemapped",
                     "_no_feature", "_ambiguous",
                     "_total"]
## Dev ##

#####################
## Snakemake rules ##
#####################

workdir: DIR_PROJ

rule all:
    input:
        # Expression Matrix per experiment/sample/plate
        csv = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.csv'),
                     expid=list(set(sample_list))),
        hdf = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.h5'),
                     expid=list(set(sample_list))),
        # Expression Matrix per item/pair-of-reads/lane
        csv_item = expand(join_path(DIR_PROJ, SUBDIR_EXPR,
                                    '{expid}', '{itemid}', 'expr.csv'), zip,
                          expid=sample_list, itemid=item_names),
        hdf_item = expand(join_path(DIR_PROJ, SUBDIR_EXPR,
                                    '{expid}', '{itemid}', 'expr.h5'), zip,
                          expid=sample_list, itemid=item_names),
        # Diagnose of demultiplexing
        demultiplexing = expand(join_path(DIR_PROJ, SUBDIR_DIAG,
                                         '{itemid}', 'demultiplexing.log'),
                                itemid=item_names),
        # Diagnose of alignment
        alignment = expand(join_path(DIR_PROJ, SUBDIR_DIAG,
                                    '{itemid}', 'alignment_diagnose.csv'),
                          itemid=item_names),
    run:
        print_logger('Expression UMI matrix is saved at {}'.format(input.csv))


rule setup_dir:
    input: config.get('SAMPLE_SHEET')
    output:
        touch('_done_setupdir'),
        dir1=SUBDIRS,
        dir2=expand(join_path('{subdir}', '{itemid}'),
               subdir=[SUBDIR_INPUT, SUBDIR_FASTQ, SUBDIR_ALIGN, SUBDIR_DIAG,
                       SUBDIR_UMI_CNT, SUBDIR_UMI_SET, SUBDIR_LOG],
               itemid=item_names),
        dir3=expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', '{itemid}'),
                    zip, expid=sample_list, itemid=item_names),

    message: 'Setting up project directory.'
    run:
        for d in output.dir1:
            mkfolder(d)
        for d in output.dir2:
            mkfolder(d)
        for d in output.dir3:
            mkfolder(d)


# Combo-demultiplexing
rule combo_demultiplexing:
    input: '_done_setupdir'
    output:
        dynamic(join_path(DIR_PROJ, SUBDIR_FASTQ, '{itemid}', '{bc}.fastq'))
    message: 'Performing combo-demultiplexing'
    run:
        # Demultiplx fastq one-by-one
        for itemid, itembc, itemr1, itemr2 in zip(item_names, bc_used, R1, R2):
            itemid_in=join_path(DIR_PROJ, SUBDIR_INPUT, itemid)
            try:
                os.symlink(itemr1, join_path(itemid_in, 'R1.fastq.gz'))
                os.symlink(itemr2, join_path(itemid_in, 'R2.fastq.gz'))
            except OSError:
                pass
            itemid_fqs_dir=join_path(DIR_PROJ, SUBDIR_FASTQ, itemid)
            itemid_log=join_path(DIR_PROJ, SUBDIR_DIAG, itemid,
                                 'demultiplexing.log')
            print_logger('Demultiplexing {}'.format(itemid))
            shell(
                """
                bc_demultiplex \
                {itemr1} \
                {itemr2} \
                --bc-index {BC_INDEX_FPATH} \
                --bc-index-used {itembc} \
                --min-bc-quality {FASTQ_QUAL_MIN_OF_BC} \
                --umi-length {UMI_LENGTH} \
                --bc-length {BC_LENGTH} \
                --cut-length {CUT_LENGTH} \
                --out-dir  {itemid_fqs_dir} \
                --is-gzip \
                --stats-file {itemid_log}
                """)
        shell('touch _done_combodemultiplex')


## Alignment ##
rule align_bowtie2:
    input:
        fq=join_path(DIR_PROJ, SUBDIR_FASTQ, '{itemid}', '{bc}.fastq'),
    output:
        sam=join_path(DIR_PROJ, SUBDIR_ALIGN, '{itemid}', '{bc}.sam')
    threads: num_threads
    log:
        join_path(DIR_PROJ, SUBDIR_LOG, '{itemid}',
                  'Align-Bowtie2_Cell-{bc}.log')
    run:
        shell(
            """
            {BOWTIE2} \
            -p {threads} \
            -x {BOWTIE2_INDEX_PREFIX} \
            -U {input.fq} \
            -S {output.sam} 2>{log} \
            --seed 42
            """)


## HT-seq Count UMI ##
rule cook_annotation:
    input: GFF
    output:
        anno=join_path(DIR_PROJ, SUBDIR_ANNO,
                         base_name(GFF) + '.pickle'),
        flag=touch('_done_annotation'),
    message: 'Cooking Annotation'
    run:
        _=cook_anno_model(GFF, feature_atrr='gene_id',
                            feature_type='exon',
                            stranded=True,
                            dumpto=output.anno,
                            verbose=verbose)


rule count_umi:
    input:
        gff=join_path(DIR_PROJ, SUBDIR_ANNO,
                        base_name(GFF) + '.pickle'),
        sam=join_path(DIR_PROJ, SUBDIR_ALIGN, '{itemid}', '{bc}.sam'),
    output:
        umi=join_path(DIR_PROJ, SUBDIR_UMI_CNT, '{itemid}', '{bc}_umi.pkl'),
        umiset=join_path(DIR_PROJ, SUBDIR_UMI_SET, '{itemid}', '{bc}_umiset.pkl'),
        aln_diag=join_path(DIR_PROJ, SUBDIR_DIAG, '{itemid}', '{bc}_aln.pkl'),
    message: 'Counting {input.sam}'
    run:
        features_f, all_genes=pickle.load(open(input.gff, 'rb'))
        all_genes=sorted(all_genes)
        umi_cnt, umi_set, aln_cnt=count_umi(sam_fpath=input.sam,
                                     features=features_f,
                                     len_umi=UMI_LENGTH,
                                     accept_aln_qual_min=ALN_QUAL_MIN,
                                     dumpto=None)
        pickle.dump(umi_cnt, open(output.umi, 'wb'))
        pickle.dump(umi_set, open(output.umiset, 'wb'))
        pickle.dump(aln_cnt, open(output.aln_diag, 'wb'))


## to-do
# - regular umi-count using *_umi.pkl -> assess technical replicates/lanes per plate
# - merge umi-count using *_umiset.pkl -> correct umi count per experiment/plate
rule umi_matrix:
    input:
        gff=join_path(DIR_PROJ, SUBDIR_ANNO,
                        base_name(GFF) + '.pickle'),
        umi=dynamic(join_path(DIR_PROJ, SUBDIR_UMI_CNT,
                                '{itemid}', '{bc}.pkl')),
    output:
        csv=expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.csv'),
                     expid=list(set(sample_list))),
        hdf=expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.h5'),
                     expid=list(set(sample_list))),
    run:
        all_genes=sorted(list(storage.fetch("anno")[1]))

        item_names=list(SAMPLE_SHEET.index)
        sample_list=SAMPLE_SHEET['SAMPLE_NAME'].values
        R1=SAMPLE_SHEET['R1'].values
        R2=SAMPLE_SHEET['R2'].values
        bc_used=SAMPLE_SHEET['CELL_BARCODES_INDEX'].values

        exp_expr_dict={}  # experiment -> expression_matrix
        for exp_id in list(set(sample_list)):
            exp_expr_dict[exp_id]={}

        for f in input.umi:
            bc_name=base_name(f)  # BC-1-xxx
            item_id=base_name(dir_name(f))  # item-1
            exp_id=SAMPLE_SHEET.loc[item_id, 'SAMPLE_NAME']

            expr_dict=exp_expr_dict.get(exp_id, {})

            bc_umi_stream=pickle.load(open(f, 'rb'))
            bc_umi_cached=expr_dict.get(bc_name, Counter())
            expr_dict[bc_name]=bc_umi_cached + bc_umi_stream

        for exp_id, expr_dict in exp_expr_dict.items():
            for bc, cnt in expr_dict.items():
                expr_dict[bc]=pd.Series([cnt[x] for x in all_genes],
                                          index=all_genes)
            expr_df=pd.DataFrame(expr_dict, index=all_genes).fillna(0)
            expr_df.to_csv(join_path(DIR_PROJ, SUBDIR_EXPR,
                                     exp_id, 'expr.csv'))
            expr_df.to_hdf(join_path(DIR_PROJ, SUBDIR_EXPR,
                                     exp_id, 'expr.h5'), 'table')

        shell('touch _done_umimatrix')

        # aln_dict = {}
        #     aln_dict[cell] = pd.Series([aln_cnt[x] for x in aln_diagnose_item],
        #                                index=aln_diagnose_item)
        # diagnose_aln = pd.DataFrame(
        #     aln_dict, index=aln_diagnose_item).fillna(0)
        # diagnose_aln.to_csv(output.diagnose)


rule cleanall:
    message: "Remove all files under {DIR_PROJ}"
    run:
        for d in SUBDIRS:
            rmfolder(join_path(DIR_PROJ, d))
        shell('rm -f _done_*')
        shell('rm -f */_done_*')

rule slim:
    input:
        # Expression Matrix
        csv=expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.csv'),
                     expid=list(set(sample_list))),
        hdf=expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.h5'),
                     expid=list(set(sample_list))),
    message: "Remove files except expression results."
    run:
        for d in [SUBDIR_FASTQ, SUBDIR_ALIGN, SUBDIR_INPUT]:
            rmfolder(join_path(DIR_PROJ, d))
        shell('rm -f _done_*')
        shell('rm -f */_done_*')
