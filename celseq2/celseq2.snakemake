######################################################################
## Dependencies ##
from celseq2.helper import mkfolder, join_path, base_name, dir_name, print_logger
from celseq2.helper import is_nonempty_file, resetfolder, rmfolder
from celseq2.helper import cook_sample_sheet
from celseq2.prepare_annotation_model import cook_anno_model
from celseq2.count_umi import count_umi
import pandas as pd
import glob
from pytools.persistent_dict import PersistentDict
import pickle
from collections import Counter

import shutil
import os

## Inforamtion ##
PROJECT_NAME = config.get('PROJECT_NAME', None)  # 'celseq2_demo'
# '/ifs/home/yy1533/Lab/cel-seq-pipe/demo/celseq2'
DIR_PROJ = config.get('DIR_PROJ', None)

## Sample Sheet ##
SAMPLE_SHEET = cook_sample_sheet(config.get('SAMPLE_SHEET', None))  # ''

## CEL-seq2 Tech Setting ##
# '/ifs/data/yanailab/refs/barcodes/barcodes_cel-seq_umis96.tab'
BC_INDEX_FPATH = config.get('BC_INDEX_FPATH', None)
BC_IDs_DEFAULT = config.get('BC_IDs_DEFAULT', None)  # '1-96'
UMI_LENGTH = config.get('UMI_LENGTH', None)  # 6
BC_LENGTH = config.get('BC_LENGTH', None)  # 6

## Tools ##
# '/ifs/data/yanailab/refs/danio_rerio/danRer10_87/genome/Danio_rerio.GRCz10.dna.toplevel'
BOWTIE2_INDEX_PREFIX = config.get('BOWTIE2_INDEX_PREFIX', None)
BOWTIE2 = config.get('BOWTIE2', None)  # '/local/apps/bowtie2/2.3.1/bowtie2'

## Annotations ##
# '/ifs/data/yanailab/refs/danio_rerio/danRer10_87/gtf/Danio_rerio.GRCz10.87.gtf.gz'
GFF = config.get('GFF', None)

## Demultiplexing ##
FASTQ_QUAL_MIN_OF_BC = config.get('FASTQ_QUAL_MIN_OF_BC', None)  # 10
CUT_LENGTH = config.get('CUT_LENGTH', None)  # 35
## Alignment ##
ALIGNER = config.get('ALIGNER', None)  # 'bowtie2'

## UMI Count ##
ALN_QUAL_MIN = config.get('ALN_QUAL_MIN', None)  # 0


## Running Parameters ##
# bc_ids_used=config.get('bc_ids_used', None)  # '1,2,3,4-5'
num_threads = config.get('num_threads', None)  # 5
verbose = config.get('verbose', None)  # True


#######################
## Pipeline reserved ##
#######################
item_names = list(SAMPLE_SHEET.index)
# expand('{r1_fpath}', r1_fpath=SAMPLE_SHEET['R1'].values)
sample_list = SAMPLE_SHEET['SAMPLE_NAME'].values
# expand('{r2_fpath}', r2_fpath=SAMPLE_SHEET['R2'].values)
R1 = SAMPLE_SHEET['R1'].values
R2 = SAMPLE_SHEET['R2'].values
bc_used = SAMPLE_SHEET['CELL_BARCODES_INDEX'].values


if not DIR_PROJ:
    print_logger('Please specify configuration for pipeline\n')
    exit(1)
if R1 is None or R2 is None:
    print_logger('Please specify reads file\n')
    exit(1)

SUBDIR_INPUT = 'input'
SUBDIR_FASTQ = 'smallfq'
SUBDIR_ALIGN = 'smallsam'
SUBDIR_UMI = 'smallumi'
SUBDIR_EXPR = 'expr'
SUBDIR_LOG = 'smalllog'
SUBDIR_QSUB = 'qsub_log'
SUBDIR_DIAG = 'smalldiagnose'
SUBDIR_ANNO = 'annotation'
SUBDIRS = [SUBDIR_INPUT,
           SUBDIR_FASTQ, SUBDIR_ALIGN, SUBDIR_UMI,
           SUBDIR_EXPR,
           SUBDIR_LOG, SUBDIR_QSUB, SUBDIR_DIAG, SUBDIR_ANNO
           ]

aln_diagnose_item = ["_unmapped",
                     "_low_map_qual", '_multimapped', "_uniquemapped",
                     "_no_feature", "_ambiguous",
                     "_total"]
## Dev ##
storage = PersistentDict("store_annotation")

#####################
## Snakemake rules ##
#####################

workdir: DIR_PROJ

rule all:
    input:
        # Expression Matrix
        csv = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.csv'),
                     expid=list(set(sample_list))),
        hdf = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.h5'),
                     expid=list(set(sample_list))),
        # Diagnose of alignment
        # join_path(DIR_PROJ, SUBDIR_DIAG, PROJECT_NAME + '_alignment.csv')
    run:
        print_logger('Expression UMI matrix is saved at {}'.format(input.csv))


rule setup_dir:
    output:
        SUBDIRS,
        expand(join_path('{subdir}', '{itemid}'),
               subdir=[SUBDIR_INPUT, SUBDIR_FASTQ, SUBDIR_ALIGN,
                       SUBDIR_UMI, SUBDIR_LOG],
               itemid=item_names),
        expand(join_path(SUBDIR_EXPR, '{expid}'),
               expid=list(set(sample_list))),
        expand(join_path(SUBDIR_DIAG, '{expid}'),
               expid=list(set(sample_list))),

    message: 'Setting up project directory.'
    run:
        for d in output:
            mkfolder(d)

rule prepare_input:
    output:
        # dynamic as 2nd solution
        r1 = expand(join_path(SUBDIR_INPUT, '{itemid}', 'R1.fastq.gz'),
                    itemid=item_names),
        r2 = expand(join_path(SUBDIR_INPUT, '{itemid}', 'R2.fastq.gz'),
                    itemid=item_names),
        bc = expand(join_path(SUBDIR_INPUT, '{itemid}', 'bc_used.txt'),
                    itemid=item_names),
        flag = expand(join_path(SUBDIR_INPUT, '{itemid}', '_done_input'),
                      itemid=item_names)
    run:
        for itemid, itembc, itemr1, itemr2 in zip(item_names, bc_used, R1, R2):
            itemid_dir = join_path(DIR_PROJ, SUBDIR_INPUT, itemid)
            os.symlink(itemr1, join_path(itemid_dir, 'R1.fastq.gz'))
            os.symlink(itemr2, join_path(itemid_dir, 'R2.fastq.gz'))
            with open(join_path(itemid_dir, 'bc_used.txt'), 'w') as fout:
                fout.write(itembc)
            touch(join_path(itemid_dir, '_done_input'))


## Demultiplexing ##
# rule demultiplexing:
#     input:
#         rules.prepare_input.flag
#     output:
rule demultiplexing:
    input:
        r1 = join_path(DIR_PROJ, SUBDIR_INPUT, '{itemid}', 'R1.fastq.gz'),
        r2 = join_path(DIR_PROJ, SUBDIR_INPUT, '{itemid}', 'R2.fastq.gz'),
        bc = join_path(DIR_PROJ, SUBDIR_INPUT, '{itemid}', 'bc_used.txt'),
        flag = join_path(SUBDIR_INPUT, '{itemid}', '_done_input')
    output:
        # fqs = dynamic(join_path(DIR_PROJ, SUBDIR_FASTQ, '{itemid_bc}.fastq')),
        fqs = dynamic(join_path(DIR_PROJ, SUBDIR_FASTQ,
                                '{itemid}', '{bc}.fastq')),
        flag = join_path(DIR_PROJ, SUBDIR_FASTQ,
                         '{itemid}', '_done_demultiplex')
    run:
        savetodir = dir_name(input.r1)
        with open(input.bc) as fin:
            itembc = readline(fin).strip()

        shell(
            """
            bc_demultiplex \
            {input.r1} \
            {input.r2} \
            --bc-index {BC_INDEX_FPATH} \
            --bc-index-used {itembc} \
            --min-bc-quality {FASTQ_QUAL_MIN_OF_BC} \
            --umi-length {UMI_LENGTH} \
            --bc-length {BC_LENGTH} \
            --cut-length {CUT_LENGTH} \
            --out-dir  {savetodir} \
            --is-gzip \
            """)
        touch(output.flag)


## Alignment ##
rule align_bowtie2:
    input:
        fq = join_path(DIR_PROJ, SUBDIR_FASTQ, '{itemid}', '{bc}.fastq'),
        flag = join_path(DIR_PROJ, SUBDIR_FASTQ,
                         '{itemid}', '_done_demultiplex')
    output:
        sam = join_path(DIR_PROJ, SUBDIR_ALIGN, '{itemid}', '{bc}.sam')
    threads: num_threads
    log:
        join_path(DIR_PROJ, SUBDIR_LOG, '{itemid}',
                  'Align-Bowtie2_Cell-{bc}.log')
    run:
        shell(
            """
            {BOWTIE2} \
            -p {threads} \
            -x {BOWTIE2_INDEX_PREFIX} \
            -U {input.fq} \
            -S {output.sam} 2>{log} \
            --seed 42
            """)


## HT-seq Count UMI ##
rule cook_annotation:
    input: GFF
    output:
        anno = temp(join_path(DIR_PROJ, SUBDIR_ANNO,
                              base_name(GFF) + '.pickle'))
    run:
        v = cook_anno_model(GFF, feature_atrr='gene_id',
                            feature_type='exon',
                            stranded=True,
                            dumpto=output.anno,
                            verbose=verbose)
        storage.store("anno", v)


rule count_umi:
    input:
        gff = rules.cook_annotation.output.anno,
        sam = join_path(DIR_PROJ, SUBDIR_ALIGN, '{itemid}', '{bc}.sam'),
    output:
        umi = join_path(DIR_PROJ, SUBDIR_UMI, '{itemid}', '{bc}.pkl'),
        aln_diag = join_path(DIR_PROJ, SUBDIR_UMI, '{itemid}', '{bc}.diag'),
    message: 'Counting {input.sam}'
    run:
        features_f = storage.fetch("anno")[0]
        all_genes = sorted(list(storage.fetch("anno")[1]))
        umi_cnt, aln_cnt = count_umi(sam_fpath=input.sam,
                                     features=features_f,
                                     len_umi=UMI_LENGTH,
                                     accept_aln_qual_min=ALN_QUAL_MIN,
                                     dumpto=None)
        pickle.dump(umi_cnt, open(output.umi, 'wb'))
        pickle.dump(aln_cnt, open(output.aln_diag, 'wb'))


rule umi_matrix:
    input:
        gff = join_path(DIR_PROJ, SUBDIR_ANNO,
                        base_name(GFF) + '.pickle'),
        umi = dynamic(join_path(DIR_PROJ, SUBDIR_UMI,
                                '{itemid}', '{bc}.pkl')),
    output:
        csv = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.csv'),
                     expid=list(set(sample_list))),
        hdf = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.h5'),
                     expid=list(set(sample_list))),
    run:
        all_genes = sorted(list(storage.fetch("anno")[1]))

        item_names = list(SAMPLE_SHEET.index)
        sample_list = SAMPLE_SHEET['SAMPLE_NAME'].values
        R1 = SAMPLE_SHEET['R1'].values
        R2 = SAMPLE_SHEET['R2'].values
        bc_used = SAMPLE_SHEET['CELL_BARCODES_INDEX'].values

        exp_expr_dict = {}  # experiment -> expression_matrix
        for exp_id in list(set(sample_list)):
            exp_expr_dict[exp_id] = {}

        for f in input.umi:
            bc_name = base_name(f)  # BC-1-xxx
            item_id = base_name(dir_name(f))  # item-1
            exp_id = SAMPLE_SHEET.loc[item_id, 'SAMPLE_NAME']

            expr_dict = exp_expr_dict.get(exp_id, {})

            bc_umi_stream = pickle.load(open(f, 'rb'))
            bc_umi_cached = expr_dict.get(bc_name, Counter())
            expr_dict[bc_name] = bc_umi_cached + bc_umi_stream

        for exp_id, expr_dict in exp_expr_dict.items():
            for bc, cnt in expr_dict.items():
                expr_dict[bc] = pd.Series([cnt[x] for x in all_genes],
                                          index=all_genes)
            expr_df = pd.DataFrame(expr_dict, index=all_genes).fillna(0)
            expr_df.to_csv(join_path(DIR_PROJ, SUBDIR_EXPR,
                                     exp_id, 'expr.csv'))
            expr_df.to_hdf(join_path(DIR_PROJ, SUBDIR_EXPR,
                                     exp_id, 'expr.h5'),
                           format='table', append=True)

        # aln_dict = {}
        #     aln_dict[cell] = pd.Series([aln_cnt[x] for x in aln_diagnose_item],
        #                                index=aln_diagnose_item)
        # diagnose_aln = pd.DataFrame(
        #     aln_dict, index=aln_diagnose_item).fillna(0)
        # diagnose_aln.to_csv(output.diagnose)


rule cleanall:
    run:
        for d in SUBDIRS:
            rmfolder(join_path(DIR_PROJ, d))
        os.remove('.done.setup')
