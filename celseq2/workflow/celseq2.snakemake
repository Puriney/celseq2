######################################################################
## Dependencies ##
from celseq2.helper import join_path, base_name, dir_name, print_logger
from celseq2.helper import rmfolder, mkfolder
from celseq2.helper import cook_sample_sheet, popen_communicate
from celseq2.prepare_annotation_model import cook_anno_model
from celseq2.count_umi import count_umi, _flatten_umi_set
import pandas as pd
import glob
import pickle

from collections import Counter, defaultdict
from multiprocessing import Pool, cpu_count

import shutil
import os

## Inforamtion ##
# '/ifs/home/yy1533/Lab/cel-seq-pipe/demo/celseq2'
DIR_PROJ = config.get('output_dir', None)

## Sample Sheet ##
SAMPLE_TABLE_FPATH = config.get('experiment_table', None)
SAMPLE_TABLE = cook_sample_sheet(SAMPLE_TABLE_FPATH)  # ''

## CEL-seq2 Tech Setting ##
# '/ifs/data/yanailab/refs/barcodes/barcodes_cel-seq_umis96.tab'
BC_INDEX_FPATH = config.get('BC_INDEX_FPATH', None)
BC_IDs_DEFAULT = config.get('BC_IDs_DEFAULT', None)  # '1-96'
BC_SEQ_COLUMN = config.get('BC_SEQ_COLUMN', None)
UMI_START_POSITION = config.get('UMI_START_POSITION', None)
BC_START_POSITION = config.get('BC_START_POSITION', None)
UMI_LENGTH = config.get('UMI_LENGTH', None)  # 6
BC_LENGTH = config.get('BC_LENGTH', None)  # 6

## Tools ##
# '/ifs/data/yanailab/refs/danio_rerio/danRer10_87/genome/Danio_rerio.GRCz10.dna.toplevel'
BOWTIE2_INDEX_PREFIX = config.get('BOWTIE2_INDEX_PREFIX', None)
BOWTIE2 = config.get('BOWTIE2', None)  # '/local/apps/bowtie2/2.3.1/bowtie2'

## Annotations ##
# '/ifs/data/yanailab/refs/danio_rerio/danRer10_87/gtf/Danio_rerio.GRCz10.87.gtf.gz'
GFF = config.get('GFF', None)

## Demultiplexing ##
FASTQ_QUAL_MIN_OF_BC = config.get('FASTQ_QUAL_MIN_OF_BC', None)  # 10
CUT_LENGTH = config.get('CUT_LENGTH', None)  # 35
## Alignment ##
ALIGNER = config.get('ALIGNER', None)  # 'bowtie2'

## UMI Count ##
ALN_QUAL_MIN = config.get('ALN_QUAL_MIN', None)  # 0
STRANDED = config.get('stranded', 'yes')


## Running Parameters ##
# bc_ids_used=config.get('bc_ids_used', None)  # '1,2,3,4-5'
num_threads = config.get('num_threads', None)  # 5
verbose = config.get('verbose', None)  # True


#######################
## Pipeline reserved ##
#######################
item_names = list(SAMPLE_TABLE.index)
# expand('{r1_fpath}', r1_fpath=SAMPLE_TABLE['R1'].values)
sample_list = SAMPLE_TABLE['SAMPLE_NAME'].values
# expand('{r2_fpath}', r2_fpath=SAMPLE_TABLE['R2'].values)
R1 = SAMPLE_TABLE['R1'].values
R2 = SAMPLE_TABLE['R2'].values
bc_used = SAMPLE_TABLE['CELL_BARCODES_INDEX'].values


if not DIR_PROJ:
    print_logger('Please specify configuration for pipeline\n')
    exit(1)
if R1 is None or R2 is None:
    print_logger('Please specify reads file\n')
    exit(1)

SUBDIR_INPUT = 'input'
SUBDIR_FASTQ = 'small_fq'
SUBDIR_ALIGN = 'small_sam'
SUBDIR_UMI_CNT = 'small_umi_count'
SUBDIR_UMI_SET = 'small_umi_set'
SUBDIR_EXPR = 'expr'
SUBDIR_LOG = 'small_log'
SUBDIR_QSUB = 'qsub_log'
SUBDIR_DIAG = 'small_diagnose'
SUBDIR_ANNO = 'annotation'
SUBDIRS = [SUBDIR_INPUT,
           SUBDIR_FASTQ, SUBDIR_ALIGN, SUBDIR_UMI_CNT, SUBDIR_UMI_SET,
           SUBDIR_EXPR,
           SUBDIR_LOG, SUBDIR_QSUB, SUBDIR_DIAG, SUBDIR_ANNO
           ]

aln_diagnose_item = ["_unmapped",
                     "_low_map_qual", '_multimapped', "_uniquemapped",
                     "_no_feature", "_ambiguous",
                     "_total"]
## Dev ##

#####################
## Snakemake rules ##
#####################

workdir: DIR_PROJ


rule all:
    message: 'Finished UMI matrix'
    input:
        # Expression Matrix per experiment/sample/plate
        csv = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.csv'),
                     expid=list(set(sample_list))),
        hdf = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.h5'),
                     expid=list(set(sample_list))),
        # Expression Matrix per item/pair-of-reads/lane
        csv_item = expand(join_path(DIR_PROJ, SUBDIR_EXPR,
                                    '{expid}', '{itemid}', 'expr.csv'), zip,
                          expid=sample_list, itemid=item_names),
        hdf_item = expand(join_path(DIR_PROJ, SUBDIR_EXPR,
                                    '{expid}', '{itemid}', 'expr.h5'), zip,
                          expid=sample_list, itemid=item_names),
        # Diagnose of demultiplexing
        # demultiplexing = expand(join_path(DIR_PROJ, SUBDIR_DIAG,
        #                                  '{itemid}', 'demultiplexing.log'),
        #                         itemid=item_names),
        # # Diagnose of alignment
        alignment = expand(join_path(DIR_PROJ, SUBDIR_DIAG,
                                     '{itemid}', 'alignment_diagnose.csv'),
                           itemid=item_names),
    output:
        touch('_done_UMI')
    run:
        try:
            shell('mv -f celseq2_job*.sh* {}'.format(SUBDIR_QSUB))
        except:
            pass
        print_logger('Expression UMI matrix is saved at {}'.format(input.csv))


rule setup_dir:
    input: SAMPLE_TABLE_FPATH
    output:
        touch('_done_setupdir'),
        dir1 = SUBDIRS,
        dir2 = expand(join_path('{subdir}', '{itemid}'),
                      subdir=[SUBDIR_INPUT,
                              SUBDIR_FASTQ, SUBDIR_ALIGN, SUBDIR_DIAG,
                              SUBDIR_UMI_CNT, SUBDIR_UMI_SET, SUBDIR_LOG],
                      itemid=item_names),
        dir3 = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', '{itemid}'),
                      zip, expid=sample_list, itemid=item_names),

    message: 'Setting up project directory.'
    run:
        for d in output.dir1:
            mkfolder(d)
        for d in output.dir2:
            mkfolder(d)
        for d in output.dir3:
            mkfolder(d)


# Combo-demultiplexing
rule combo_demultiplexing:
    input: '_done_setupdir'
    output:
        dynamic(join_path(DIR_PROJ, SUBDIR_FASTQ, '{itemid}', '{bc}.fastq')),
    message: 'Performing combo-demultiplexing'
    threads: len(item_names)
    run:
        # Demultiplx fastq in Process pool
        p = Pool(threads)
        for itemid, itembc, itemr1, itemr2 in zip(item_names, bc_used, R1, R2):
            itemid_in = join_path(DIR_PROJ, SUBDIR_INPUT, itemid)
            try:
                os.symlink(itemr1, join_path(itemid_in, 'R1.fastq.gz'))
                os.symlink(itemr2, join_path(itemid_in, 'R2.fastq.gz'))
            except OSError:
                pass
            itemid_fqs_dir = join_path(DIR_PROJ, SUBDIR_FASTQ, itemid)
            itemid_log = join_path(DIR_PROJ, SUBDIR_DIAG, itemid,
                                   'demultiplexing.log')
            print_logger('Demultiplexing {}'.format(itemid))
            cmd = " ".join(["bc_demultiplex",
                            itemr1,
                            itemr2,
                            "--bc-index {}".format(BC_INDEX_FPATH),
                            "--bc-seq-column {}".format(BC_SEQ_COLUMN),
                            "--bc-index-used {}".format(itembc),
                            "--min-bc-quality {}".format(FASTQ_QUAL_MIN_OF_BC),
                            "--umi-start-position {}".format(
                                UMI_START_POSITION),
                            "--bc-start-position {}".format(BC_START_POSITION),
                            "--umi-length {}".format(UMI_LENGTH),
                            "--bc-length {}".format(BC_LENGTH),
                            "--cut-length {}".format(CUT_LENGTH),
                            "--out-dir  {}".format(itemid_fqs_dir),
                            "--is-gzip ",
                            "--stats-file {}".format(itemid_log)])
            p.apply_async(shell, args=(cmd,))
        p.close()
        p.join()
        shell('touch _done_combodemultiplex')


## Alignment ##
rule align_bowtie2:
    input:
        fq = join_path(DIR_PROJ, SUBDIR_FASTQ, '{itemid}', '{bc}.fastq'),
    output:
        sam = join_path(DIR_PROJ, SUBDIR_ALIGN, '{itemid}', '{bc}.sam')
    threads: num_threads
    log:
        join_path(DIR_PROJ, SUBDIR_LOG, '{itemid}',
                  'Align-Bowtie2_Cell-{bc}.log')
    run:
        shell(
            """
            {BOWTIE2} \
            -p {threads} \
            -x {BOWTIE2_INDEX_PREFIX} \
            -U {input.fq} \
            -S {output.sam} 2>{log} \
            --seed 42
            """)


## HT-seq Count UMI ##
rule cook_annotation:
    input: GFF
    output:
        anno = join_path(DIR_PROJ, SUBDIR_ANNO,
                         base_name(GFF) + '.pickle'),
        flag = touch('_done_annotation'),
    message: 'Cooking Annotation'
    run:
        _ = cook_anno_model(GFF, feature_atrr='gene_id',
                            feature_type='exon',
                            stranded=True,
                            dumpto=output.anno,
                            verbose=verbose)


rule count_umi:
    input:
        gff = join_path(DIR_PROJ, SUBDIR_ANNO,
                        base_name(GFF) + '.pickle'),
        sam = join_path(DIR_PROJ, SUBDIR_ALIGN, '{itemid}', '{bc}.sam'),
    output:
        umicnt = join_path(DIR_PROJ, SUBDIR_UMI_CNT, '{itemid}', '{bc}.pkl'),
        umiset = join_path(DIR_PROJ, SUBDIR_UMI_SET, '{itemid}', '{bc}.pkl'),
        aln_diag = join_path(DIR_PROJ, SUBDIR_DIAG, '{itemid}', '{bc}.pkl'),
    message: 'Counting {input.sam}'
    run:
        features_f, all_genes = pickle.load(open(input.gff, 'rb'))
        all_genes = sorted(all_genes)
        umi_cnt, umi_set, aln_cnt = count_umi(sam_fpath=input.sam,
                                              features=features_f,
                                              len_umi=UMI_LENGTH,
                                              stranded=STRANDED,
                                              accept_aln_qual_min=ALN_QUAL_MIN,
                                              dumpto=None)
        pickle.dump(umi_cnt, open(output.umicnt, 'wb'))
        pickle.dump(umi_set, open(output.umiset, 'wb'))
        pickle.dump(aln_cnt, open(output.aln_diag, 'wb'))


# - regular umi-count using *_umicnt.pkl -> umi_count_matrix replicates/lanes per plate
rule summarize_umi_matrix_per_item:
    input:
        gff = join_path(DIR_PROJ, SUBDIR_ANNO,
                        base_name(GFF) + '.pickle'),
        umicnt = dynamic(join_path(DIR_PROJ, SUBDIR_UMI_CNT,
                                   '{itemid}', '{bc}.pkl')),
    output:
        # Expression Matrix per item/pair-of-reads/lane per sample/plate
        csv_item = expand(join_path(DIR_PROJ, SUBDIR_EXPR,
                                    '{expid}', '{itemid}', 'expr.csv'), zip,
                          expid=sample_list, itemid=item_names),
        hdf_item = expand(join_path(DIR_PROJ, SUBDIR_EXPR,
                                    '{expid}', '{itemid}', 'expr.h5'), zip,
                          expid=sample_list, itemid=item_names),
    run:
        _, all_genes = pickle.load(open(input.gff, 'rb'))
        all_genes = sorted(all_genes)

        # item -> dict(cell_bc -> Counter(umi_vector))
        item_expr_matrix = defaultdict(dict)

        for f in input.umicnt:
            bc_name = base_name(f)  # BC-1-xxx
            item_id = base_name(dir_name(f))  # item-1
            item_expr_matrix[item_id][bc_name] = pickle.load(open(f, 'rb'))

        for item_id, expr_dict in item_expr_matrix.items():
            exp_id = SAMPLE_TABLE.loc[item_id, 'SAMPLE_NAME']  # E1

            for bc, cnt in expr_dict.items():
                expr_dict[bc] = pd.Series([cnt[x] for x in all_genes],
                                          index=all_genes)

            expr_df = pd.DataFrame(expr_dict, index=all_genes).fillna(0)
            expr_df.to_csv(join_path(DIR_PROJ, SUBDIR_EXPR,
                                     exp_id, item_id, 'expr.csv'))
            expr_df.to_hdf(join_path(DIR_PROJ, SUBDIR_EXPR,
                                     exp_id, item_id, 'expr.h5'), 'table')

        shell('touch _done_umimatrix_per_item')


# - merge umi-count using *_umiset.pkl -> correct umi count per experiment/plate
rule umi_matrix:
    input:
        csv = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.csv'),
                     expid=list(set(sample_list))),
        hdf = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.h5'),
                     expid=list(set(sample_list))),
        alignment = expand(join_path(DIR_PROJ, SUBDIR_DIAG,
                                     '{itemid}', 'alignment_diagnose.csv'),
                           itemid=item_names),

    message: 'UMI matrix per experiment'


rule summarize_umi_matrix_per_experiment:
    input:
        gff = join_path(DIR_PROJ, SUBDIR_ANNO,
                        base_name(GFF) + '.pickle'),
        umiset = dynamic(join_path(DIR_PROJ, SUBDIR_UMI_SET,
                                   '{itemid}', '{bc}.pkl')),
    output:
        # Expression Matrix per experiment/sample/plate
        csv = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.csv'),
                     expid=list(set(sample_list))),
        hdf = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.h5'),
                     expid=list(set(sample_list))),
    run:
        _, all_genes = pickle.load(open(input.gff, 'rb'))
        all_genes = sorted(all_genes)

        sample_list = SAMPLE_TABLE['SAMPLE_NAME'].values

        # experiment_id -> dict(cell_bc -> dict(gname -> set(umi)))
        exp_expr_matrix = {}

        for exp_id in list(set(sample_list)):
            exp_expr_matrix[exp_id] = defaultdict(dict)

        for f in input.umiset:
            bc_name = base_name(f)  # BC-1-xxx
            item_id = base_name(dir_name(f))  # item-1
            exp_id = SAMPLE_TABLE.loc[item_id, 'SAMPLE_NAME']

            umiset_stream = pickle.load(open(f, 'rb'))
            if len(exp_expr_matrix[exp_id][bc_name]) == 0:
                exp_expr_matrix[exp_id][bc_name] = umiset_stream
                continue
            umiset_cached = exp_expr_matrix[exp_id][bc_name]
            for x in all_genes:
                y1 = exp_expr_matrix[exp_id][bc_name].get(x, set())
                y2 = umiset_stream.get(x, set())
                exp_expr_matrix[exp_id][bc_name][x] = y1 | y2

        for exp_id, expr_dict in exp_expr_matrix.items():
            for bc, cnt in expr_dict.items():
                cnt = _flatten_umi_set(cnt)
                expr_dict[bc] = pd.Series([cnt[x] for x in all_genes],
                                          index=all_genes)
            expr_df = pd.DataFrame(expr_dict, index=all_genes).fillna(0)
            expr_df.to_csv(join_path(DIR_PROJ, SUBDIR_EXPR,
                                     exp_id, 'expr.csv'))
            expr_df.to_hdf(join_path(DIR_PROJ, SUBDIR_EXPR,
                                     exp_id, 'expr.h5'), 'table')

        shell('touch _done_umimatrix_per_experiment')


rule summarize_alignment_diagnose:
    input:
        aln_diag = dynamic(join_path(DIR_PROJ, SUBDIR_DIAG,
                                     '{itemid}', '{bc}.pkl')),
    output:
        # Diagnose of alignment
        alignment = expand(join_path(DIR_PROJ, SUBDIR_DIAG,
                                     '{itemid}', 'alignment_diagnose.csv'),
                           itemid=item_names),
    run:
        # item_id -> dict(cell_bc -> Counter(align))
        item_aln_mat = defaultdict(dict)

        for f in input.aln_diag:
            bc_name = base_name(f)  # BC-1-xxx
            item_id = base_name(dir_name(f))  # item-1
            item_aln_mat[item_id][bc_name] = pickle.load(open(f, 'rb'))

        for item_id, aln_dict in item_aln_mat.items():
            for bc, cnt in aln_dict.items():
                aln_dict[bc] = pd.Series([cnt[x] for x in aln_diagnose_item],
                                         index=aln_diagnose_item)

            aln_df = pd.DataFrame(aln_dict, index=aln_diagnose_item).fillna(0)
            aln_df.to_csv(join_path(DIR_PROJ, SUBDIR_DIAG,
                                    item_id, 'alignment_diagnose.csv'))


rule cleanall:
    message: "Remove all files under {DIR_PROJ}"
    run:
        for d in SUBDIRS:
            rmfolder(join_path(DIR_PROJ, d))
        shell('rm -f _done_*')
        shell('rm -f {}'.format(join_path(SUBDIR_QSUB, 'celseq2_job.*.sh.*')))


rule clean_FQ_SAM:
    input:
        # Expression Matrix
        csv = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.csv'),
                     expid=list(set(sample_list))),
        hdf = expand(join_path(DIR_PROJ, SUBDIR_EXPR, '{expid}', 'expr.h5'),
                     expid=list(set(sample_list))),
    message: "Remove files under {DIR_PROJ} except expression results."
    run:
        for d in [SUBDIR_FASTQ, SUBDIR_ALIGN, SUBDIR_INPUT]:
            shell('rm -rf {}'.format(join_path(DIR_PROJ, d, '*')))
        shell('rm -f _done_combodemultiplex')


rule before_rerun_expr:
    run:
        for d in [SUBDIR_FASTQ, SUBDIR_ALIGN, SUBDIR_INPUT]:
            shell('rm -rf {}'.format(join_path(DIR_PROJ, d, '*')))
        shell('rm -f _done_combodemultiplex')
        shell('rm -f {}'.format(join_path(SUBDIR_QSUB, 'celseq2_job.*.sh.*')))
